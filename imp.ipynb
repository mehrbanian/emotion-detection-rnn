{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(pd.read_csv(\"Dataset/Text_Emotion_Data.csv\"))\n",
    "stopwords = np.array(pd.read_csv(\"Dataset/stopwords.txt\", sep=\" \", header=None)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, 0]\n",
    "y = data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_none_alpha(x):\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    return regex.sub('', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [remove_none_alpha(w) for w in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq = []\n",
    "for sentence in X:\n",
    "    words_seq = remove_none_alpha(sentence).lower().split(\" \")\n",
    "    words_seq = [w for w in words_seq if len(w) > 2]\n",
    "    words_seq = [w for w in words_seq if w not in stopwords]\n",
    "    X_seq.append(words_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = len(max(X_seq, key=len))\n",
    "\n",
    "# create a new list of lists with the desired size, padding with empty strings\n",
    "sequences = [['' for _ in range(max_len)] for _ in range(len(X_seq))]\n",
    "\n",
    "# copy the elements from the original lists into the new list, padding with empty strings as needed\n",
    "for i, sequence in enumerate(X_seq):\n",
    "    sequences[i][:len(sequence)] = sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "my_list = [1, 2, 3]\n",
    "threshold = 6\n",
    "\n",
    "my_list = [0] * (threshold - len(my_list)) + my_list\n",
    "\n",
    "print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot_encode(sequence, word_to_idx):\n",
    "    num_words = len(word_to_idx)\n",
    "    encoding = np.zeros((len(sequence), num_words))\n",
    "    for i, word in enumerate(sequence):\n",
    "        if word in word_to_idx:\n",
    "            encoding[i, word_to_idx[word]] = 1\n",
    "    return encoding\n",
    "\n",
    "def one_hot_encode2(sequence, word_to_idx, max_bit_num):\n",
    "    encoding = np.zeros((len(sequence), max_bit_num))\n",
    "    for i, word in enumerate(sequence):\n",
    "        if word in word_to_idx:            \n",
    "            binary_string = bin(word_to_idx[word])[2:]  # exclude the \"0b\" prefix\n",
    "            binary_array = np.array([int(digit) for digit in binary_string])\n",
    "            binary_array = np.pad(binary_array, (0, max_bit_num - len(binary_array)), 'constant')\n",
    "\n",
    "            encoding[i, :] = binary_array\n",
    "    return encoding\n",
    "\n",
    "unique_words = list(set([word for sequence in sequences for word in sequence]))\n",
    "word_to_idx = {word: idx for idx, word in enumerate(unique_words)}\n",
    "\n",
    "max_bit_num = 1\n",
    "while(True):\n",
    "    if(2**max_bit_num > len(unique_words)): break\n",
    "    max_bit_num += 1\n",
    "\n",
    "encoded_sequences = []\n",
    "for sequence in sequences:\n",
    "    encoded_sequences.append(one_hot_encode2(sequence, word_to_idx, max_bit_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([], shape=(0, 8), dtype=float64), array([[0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Flatten the list of sequences and create a pandas DataFrame\n",
    "words = pd.DataFrame([word for sequence in sequences for word in sequence], columns=['word'])\n",
    "\n",
    "# Create a OneHotEncoder object\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit the encoder to the DataFrame\n",
    "encoder.fit(words)\n",
    "\n",
    "# Transform the DataFrame using the fitted encoder\n",
    "transformed = encoder.transform(words).toarray()\n",
    "\n",
    "# Split the transformed array back into the original sequences\n",
    "offsets = [0] + [len(sequence) for sequence in sequences]\n",
    "offsets = [sum(offsets[:i]) for i in range(len(offsets))]\n",
    "transformed_sequences = [transformed[offsets[i]:offsets[i+1]] for i in range(len(offsets)-1)]\n",
    "\n",
    "# Print the transformed sequences\n",
    "print(transformed_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Abbas Mrb\\Downloads\\NNDL-Pr2\\NNDL-Pr2\\imp.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abbas%20Mrb/Downloads/NNDL-Pr2/NNDL-Pr2/imp.ipynb#X31sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m encoder\u001b[39m.\u001b[39mfit(np\u001b[39m.\u001b[39mconcatenate(encoded_sequences))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abbas%20Mrb/Downloads/NNDL-Pr2/NNDL-Pr2/imp.ipynb#X31sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Transform the encoded sequences using the fitted encoder\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Abbas%20Mrb/Downloads/NNDL-Pr2/NNDL-Pr2/imp.ipynb#X31sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m transformed_sequences \u001b[39m=\u001b[39m [encoder\u001b[39m.\u001b[39mtransform(encoded_sequence)\u001b[39m.\u001b[39mtoarray() \u001b[39mfor\u001b[39;00m encoded_sequence \u001b[39min\u001b[39;00m encoded_sequences]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abbas%20Mrb/Downloads/NNDL-Pr2/NNDL-Pr2/imp.ipynb#X31sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Print the transformed sequences\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abbas%20Mrb/Downloads/NNDL-Pr2/NNDL-Pr2/imp.ipynb#X31sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(transformed_sequences)\n",
      "\u001b[1;32mc:\\Users\\Abbas Mrb\\Downloads\\NNDL-Pr2\\NNDL-Pr2\\imp.ipynb Cell 8\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abbas%20Mrb/Downloads/NNDL-Pr2/NNDL-Pr2/imp.ipynb#X31sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m encoder\u001b[39m.\u001b[39mfit(np\u001b[39m.\u001b[39mconcatenate(encoded_sequences))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abbas%20Mrb/Downloads/NNDL-Pr2/NNDL-Pr2/imp.ipynb#X31sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Transform the encoded sequences using the fitted encoder\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Abbas%20Mrb/Downloads/NNDL-Pr2/NNDL-Pr2/imp.ipynb#X31sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m transformed_sequences \u001b[39m=\u001b[39m [encoder\u001b[39m.\u001b[39;49mtransform(encoded_sequence)\u001b[39m.\u001b[39mtoarray() \u001b[39mfor\u001b[39;00m encoded_sequence \u001b[39min\u001b[39;00m encoded_sequences]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abbas%20Mrb/Downloads/NNDL-Pr2/NNDL-Pr2/imp.ipynb#X31sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Print the transformed sequences\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abbas%20Mrb/Downloads/NNDL-Pr2/NNDL-Pr2/imp.ipynb#X31sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(transformed_sequences)\n",
      "File \u001b[1;32mc:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:509\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[39m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[0;32m    508\u001b[0m warn_on_unknown \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_unknown \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 509\u001b[0m X_int, X_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(\n\u001b[0;32m    510\u001b[0m     X,\n\u001b[0;32m    511\u001b[0m     handle_unknown\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_unknown,\n\u001b[0;32m    512\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    513\u001b[0m     warn_on_unknown\u001b[39m=\u001b[39;49mwarn_on_unknown,\n\u001b[0;32m    514\u001b[0m )\n\u001b[0;32m    516\u001b[0m n_samples, n_features \u001b[39m=\u001b[39m X_int\u001b[39m.\u001b[39mshape\n\u001b[0;32m    518\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_idx_ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:168\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[1;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown)\u001b[0m\n\u001b[0;32m    165\u001b[0m             Xi[\u001b[39m~\u001b[39mvalid_mask] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories_[i][\u001b[39m0\u001b[39m]\n\u001b[0;32m    166\u001b[0m     \u001b[39m# We use check_unknown=False, since _check_unknown was\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# already called above.\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     X_int[:, i] \u001b[39m=\u001b[39m _encode(Xi, uniques\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcategories_[i], check_unknown\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    169\u001b[0m \u001b[39mif\u001b[39;00m columns_with_unknown:\n\u001b[0;32m    170\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    171\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound unknown categories in columns \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcolumns_with_unknown\u001b[39m}\u001b[39;00m\u001b[39m during transform. These \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    173\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39munknown categories will be encoded as all zeros\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    174\u001b[0m         \u001b[39mUserWarning\u001b[39;00m,\n\u001b[0;32m    175\u001b[0m     )\n",
      "File \u001b[1;32mc:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_encode.py:190\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[39mif\u001b[39;00m diff:\n\u001b[0;32m    189\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my contains previously unseen labels: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(diff)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 190\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49msearchsorted(uniques, values)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msearchsorted\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\anaconda\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1413\u001b[0m, in \u001b[0;36msearchsorted\u001b[1;34m(a, v, side, sorter)\u001b[0m\n\u001b[0;32m   1345\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[0;32m   1346\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearchsorted\u001b[39m(a, v, side\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m, sorter\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1347\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1348\u001b[0m \u001b[39m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[0;32m   1349\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1411\u001b[0m \n\u001b[0;32m   1412\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1413\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39msearchsorted\u001b[39;49m\u001b[39m'\u001b[39;49m, v, side\u001b[39m=\u001b[39;49mside, sorter\u001b[39m=\u001b[39;49msorter)\n",
      "File \u001b[1;32mc:\\anaconda\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Create a set of all unique words in the sequences\n",
    "unique_words = set()\n",
    "for sequence in sequences:\n",
    "    unique_words.update(sequence)\n",
    "\n",
    "# Create a mapping from words to indices\n",
    "word_to_index = {word: index for index, word in enumerate(unique_words)}\n",
    "\n",
    "# Create a new list to hold the one-hot encoded vectors\n",
    "encoded_sequences = []\n",
    "\n",
    "# Encode each sequence in the list using one-hot encoding\n",
    "for sequence in sequences:\n",
    "    encoded_sequence = np.zeros((len(sequence), len(unique_words)))\n",
    "    for i, word in enumerate(sequence):\n",
    "        index = word_to_index[word]\n",
    "        encoded_sequence[i][index] = 1\n",
    "    encoded_sequences.append(encoded_sequence)\n",
    "\n",
    "# Create a OneHotEncoder object for future use\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit the encoder to the encoded sequences to obtain the categories\n",
    "encoder.fit(np.concatenate(encoded_sequences))\n",
    "\n",
    "# Transform the encoded sequences using the fitted encoder\n",
    "transformed_sequences = [encoder.transform(encoded_sequence).toarray() for encoded_sequence in encoded_sequences]\n",
    "\n",
    "# Print the transformed sequences\n",
    "print(transformed_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:-150]\n",
    "X_test = X[-150:]\n",
    "y_train = y[:-150]\n",
    "y_test = y[-150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
